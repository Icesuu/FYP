# -*- coding: utf-8 -*-
"""YOLONAS WITH CUSTOM TRAIN

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/10djQMwbhi38OiowFRPCwsKwbpaZMJHQb

# **Mouting Drive**
"""

import os, sys
from google.colab import drive
drive.mount('drive')
#nb_path = '/content/notebooks'
#os.symlink('/content/drive/My Drive/Colab Notebooks', nb_path)
#sys.path.insert(0,nb_path)

#!virtualenv /content/drive/MyDrive/vir_env
#!source /content/drive/MyDrive/vir_env/bin/activate; pip install numpy
#import sys
#sys.path.append("/content/drive/MyDrive/FYP/lib/python3.10/site-packages")

#!dir /content/drive/MyDrive/FYP

"""#**Step 01: Install the Required Packages**"""

# Commented out IPython magic to ensure Python compatibility.
# %%capture
# !pip install super-gradients
# !pip install imutils
# !pip install roboflow
# !pip install pytube --upgrade
# !pip install torchinfo
# !pip install pillow
# !pip install easyocr
# #!pip install pytesseract
# #!pip install tesseract
# !pip install super-resolution

"""#**Step 02: Import the Required Libraries**"""

from super_gradients.training import models
from super_gradients.training import Trainer
from super_gradients.training import dataloaders
from super_gradients.training.dataloaders.dataloaders import coco_detection_yolo_format_train, coco_detection_yolo_format_val
from IPython.display import clear_output
from super_gradients.training.losses import PPYoloELoss
from super_gradients.training.metrics import DetectionMetrics_050
from super_gradients.training.models.detection_models.pp_yolo_e import PPYoloEPostPredictionCallback
from PIL import Image
from torchinfo import summary
import super_gradients.training.utils as utils
import cv2
import numpy as np
import matplotlib.pyplot as plt
from google.colab.patches import cv2_imshow
from roboflow import Roboflow
import imutils
import easyocr
import time
from super_resolution import cartoon_upsampling_8x

"""#**Step 03: Setting the checkpoint directory and experiment name**

---


"""

CHECKPOINT_DIR = '/content/drive/MyDrive/FYP/checkpoints'
trainer = Trainer (experiment_name = 'apnr_yolonas_run', ckpt_root_dir=CHECKPOINT_DIR)

"""#**Step 04: Exporting dataset from ROBOFLOW**"""

'''
rf = Roboflow(api_key="D9VG7ZKN80WdwYIchywm")
project = rf.workspace("project-p5nyc").project("license-plate-wh9yk")
dataset = project.version(1).download("yolov8")'''

"""#**Step 05: Train YOLONAS**"""

dataset_params ={
    'data_dir':'/content/drive/MyDrive/FYP/License-Plate-1',
    'train_images_dir':'train/images',
    'train_labels_dir':'train/labels',
    'val_images_dir':'valid/images',
    'val_labels_dir':'valid/labels',
    'test_images_dir':'test/images',
    'test_labels_dir':'test/labels',
    'classes':['license']
}

'''dataset_params ={
    'data_dir':'/content/drive/MyDrive/FYP/ccpd_green',
    'train_images_dir':'train/images',
    'train_labels_dir':'train/labels',
    'val_images_dir':'valid/images',
    'val_labels_dir':'valid/labels',
    'test_images_dir':'test/images',
    'test_labels_dir':'test/labels',
    'classes':['license']'''

'''dataset_params ={
    'data_dir':'/content/drive/MyDrive/FYP/Mplate',
    'train_images_dir':'train/images',
    'train_labels_dir':'train/labels',
    'val_images_dir':'valid/images',
    'val_labels_dir':'valid/labels',
    'test_images_dir':'test/images',
    'test_labels_dir':'test/labels',
    'classes':['license']'''

"""#**Pass the values for dataset_params into the dataset_params argument**"""

train_data = coco_detection_yolo_format_train(
    dataset_params={
        'data_dir': dataset_params['data_dir'],
        'images_dir': dataset_params['train_images_dir'],
        'labels_dir': dataset_params['train_labels_dir'],
        'classes': dataset_params['classes']
    },
    dataloader_params={
        'batch_size':16,
        'num_workers':2
    }
)

val_data = coco_detection_yolo_format_val(
    dataset_params={
        'data_dir': dataset_params['data_dir'],
        'images_dir': dataset_params['val_images_dir'],
        'labels_dir': dataset_params['val_labels_dir'],
        'classes': dataset_params['classes']
    },
    dataloader_params={
        'batch_size':16,
        'num_workers':2
    }
)

test_data = coco_detection_yolo_format_val(
    dataset_params={
        'data_dir': dataset_params['data_dir'],
        'images_dir': dataset_params['test_images_dir'],
        'labels_dir': dataset_params['test_labels_dir'],
        'classes': dataset_params['classes']
    },
    dataloader_params={
        'batch_size':16,
        'num_workers':2
    }
)

clear_output()

"""#**Inspect the dataset defined earlier**"""

train_data.dataset.transforms

train_data.dataset.dataset_params['transforms'][1]

train_data.dataset.dataset_params['transforms'][1]['DetectionRandomAffine']['degress']=10.42

"""##Plot a batch of trainig data with their augmentations applied to see what they look like##"""

train_data.dataset.plot()

"""#**Instantiating the model**"""

model= models.get('yolo_nas_s',
                  num_classes=len(dataset_params['classes']),
                  pretrained_weights="coco"
                  )

"""##Define metrics and training parameters##

There are a few mandatory argument that need to define for training parameters

max_epochs = Max number of training epochs

loss = the loss function you want to use

optimizer = optimizer that will be using

train_metrics_list = metrics to log during training

valid_metrics_list = metrics to log during training

metric_to_watch = metric which the model checkpoints will be saved according t0o

We can choose from a variety of optimizer's such as: Adam, AdamW, SGD, Lion, or RMSProps. IF you choose to change the default parameters of these optimizers you pass them into optimizer_params
"""

train_params = {
    # Enabling silent mode
    'silent_mode': True,
    "average_best_models": True,
    "warmup_mode": "linear_epoch_step",
    "warmup_initial_lr": 1e-6,
    "lr_warmup_epochs":3,
    "initial_lr":5e-4,
    "lr_mode":"cosine",
    "cosine_final_1=lr_ratio":0.1,
    "optimizer":"Adam",
    "optimizer_params":{"weight_decay":0.0001},
    "zero_weight_decay_on_bias_and_bn":True,
    "ema":True,
    "ema_params":{"decay":0.9, "decay_type":"threshold"},
    # Only training for 10 Epochs for this example notebook
    "max_epochs" :10,
    "mixed_precision":True,
    "loss":PPYoloELoss(
        use_static_assigner = False,
        # Note: num_classes need to be defined here
        num_classes=len(dataset_params['classes']),
        reg_max=16
    ),
    "valid_metrics_list":[
        DetectionMetrics_050(
            score_thres = 0.1,
            top_k_predictions = 300,
            # Note: num_classes need to be defined here
            num_cls=len(dataset_params['classes']),
            normalize_targets=True,
            post_prediction_callback=PPYoloEPostPredictionCallback(
                score_threshold = 0.01,
                nms_top_k = 1000,
                max_predictions = 300,
                nms_threshold = 0.7
            )
        )
    ],
    "metric_to_watch":"mAP@0.50"
}

"""##Download the Demo Videos##"""

'''
!gdown "https://drive.google.com/uc?id=1crFwrpMF1OlaJ0ZCZjBNRo9llLEVR8VQ&confirm=t"
!gdown "https://drive.google.com/uc?id=1cTIBNQ1R_7JAOURVv9cJ6P935ym_IkZ0&confirm=t"
!gdown "https://drive.google.com/uc?id=1256pNK0nQnEDT6FRLQAraTRkOY7BSprq&confirm=t"
!gdown "https://drive.google.com/uc?id=15D71z_g8uxZfXSx2ya3sy4n2-eg53meH&confirm=t"
!gdown "https://drive.google.com/uc?id=1iYW9ZAsYAaHkWZhFVwQh_ch41TMt30-Q&confirm=t"
'''

"""#**Training the model**

Training a model using SuperGradients is done using the trainer.
"""

trainer.train(model=model,
              training_params = train_params,
              train_loader = train_data,
              valid_loader = val_data)

"""# **RUN FROM HERE**

#**Get the best trained model**
"""

best_model = models.get('yolo_nas_s',
                        num_classes = len(dataset_params['classes']),
                        checkpoint_path = "drive/MyDrive/checkpoints/apnr_yolonas_run/RUN_20240103_064341_571254/ckpt_best.pth")

"""##Evaluating the best trained model on the test set##"""

trainer.test(model=best_model,
            test_loader=test_data,
            test_metrics_list=DetectionMetrics_050(score_thres=0.1,
                                                   top_k_predictions=300,
                                                   num_cls=len(dataset_params['classes']),
                                                   normalize_targets=True,
                                                   post_prediction_callback=PPYoloEPostPredictionCallback(score_threshold=0.01,
                                                                                                          nms_top_k=1000,
                                                                                                          max_predictions=300,
                                                                                                          nms_threshold=0.7)
                                                  ))

"""##Predicting with the best model##"""

img_url = '/content/drive/MyDrive/FYP/content/Cars249.png'
best_model.predict(img_url).show()

"""Difficult plate"""

img_url = '/content/drive/MyDrive/FYP/content/dirty.png'
best_model.predict(img_url).show()

img_url = '/content/drive/MyDrive/FYP/content/carshow.jpg'
best_model.predict(img_url).show()

img_url = '/content/drive/MyDrive/FYP/content/carshowedited.jpg'
best_model.predict(img_url).show()

img_url = '/content/drive/MyDrive/FYP/content/muddy1.jpg'
best_model.predict(img_url).show()

img_url = '/content/drive/MyDrive/FYP/content/dirty.png'
best_model.predict(img_url).show()

import cv2
import numpy as np

original= cv2.imread('/content/drive/MyDrive/FYP/content/dirty.png', cv2.IMREAD_UNCHANGED)
sharpen_filter=np.array([[0,-1,0],
                 [-1,5,-1],
                [0,-1,0]])

sharp_image=cv2.filter2D(original,-1,sharpen_filter)
best_model.predict(sharp_image).show()

original= cv2.imread('/content/drive/MyDrive/FYP/content/dirty.png', cv2.IMREAD_UNCHANGED)
inverse_image = 255 - original

detected = best_model.predict(inverse_image)
detected.show()
#best_model.predict(inverse_image).show()

img_url = '/content/drive/MyDrive/FYP/content/211104-JPJ-fancy-plate.jpg'
detection = best_model.predict(img_url)
detection.show()

"""# **Process**"""

!pip install facelib



"""# **Testing Stat Here**"""

t1 = time.perf_counter()
img_path = '/content/3.jpg'
super_img = cartoon_upsampling_8x(img_path,'/content/super.jpg')

#plt.imshow(super_img)
#plt.show()

!pip install image_dehazer

import cv2
import math
import numpy as np
import sys
def apply_mask(matrix, mask, fill_value):

    #print(flat[60])
    #print(flat[11940])

    masked = np.ma.array(matrix, mask=mask, fill_value=fill_value)
    print('MASKED=',masked)
    return masked.filled()

def apply_threshold(matrix, low_value, high_value):
    low_mask = matrix < low_value
    matrix = apply_mask(matrix, low_mask, low_value)
    print('Low MASK->',low_mask,'\nMatrix->',matrix)

    high_mask = matrix > high_value
    matrix = apply_mask(matrix, high_mask, high_value)

    return matrix

def simplest_cb(img, percent):
    assert img.shape[2] == 3
    assert percent > 0 and percent < 100

    half_percent = percent / 200.0
    print('HALF PERCENT->',half_percent)

    channels = cv2.split(img)
    print('Channels->\n',channels)
    print('Shape->',channels[0].shape)
    print('Shape of channels->',len(channels[2]))

    out_channels = []
    for channel in channels:
        assert len(channel.shape) == 2
        # find the low and high precentile values (based on the input percentile)
        height, width = channel.shape
        vec_size = width * height
        flat = channel.reshape(vec_size)
        print('vec=',vec_size,'\nFlat=',flat)
        assert len(flat.shape) == 1

        flat = np.sort(flat)

        n_cols = flat.shape[0]

        low_val  = flat[math.floor(n_cols * half_percent)]
        high_val = flat[math.ceil( n_cols * (1.0 - half_percent))]

        print ("Lowval: ", low_val)
        print ("Highval: ", high_val)

        # saturate below the low percentile and above the high percentile
        thresholded = apply_threshold(channel, low_val, high_val)
        # scale the channel
        normalized = cv2.normalize(thresholded, thresholded.copy(), 0, 255, cv2.NORM_MINMAX)
        out_channels.append(normalized)

    return cv2.merge(out_channels)

if __name__ == '__main__':
    #img = cv2.imread(sys.argv[1])
    img = cv2.imread('3.jpg')
    out = simplest_cb(img, 1)
    #cv2_imshow("Before", img)
    #cv2_imshow("After", out)
    print(img)
    print(out)
    plt.subplot(1, 2, 1)
    plt.imshow(cv2.cvtColor(img, cv2.COLOR_BGR2RGB))
    plt.axis('off')

    plt.subplot(1, 2, 1)
plt.imshow(cv2.cvtColor(img2, cv2.COLOR_BGR2RGB))
plt.axis('off')
    #cv2_waitKey(0)

img_url = cv2.imread('/content/super.jpg', cv2.IMREAD_UNCHANGED)

#inverse_image = cv2.bitwise_not(img_url)

'''
#inverse_image = cv2.resize(inverse_image, None, fx=2, fy=2, interpolation=cv2.INTER_CUBIC)
# Adjust contrast and brightness
alpha = 2  # Contrast control (1.0 means no change)
beta = 10   # Brightness control
img_detect = cv2.convertScaleAbs(inverse_image, alpha=alpha, beta=beta)'''

img_detect = cv2.addWeighted(img_url, 1.2, img_url, 0, 0)


'''#Trying out with Canny
gray_image = cv2.cvtColor(img_detect, cv2.COLOR_BGR2GRAY)

# Apply the Canny edge detector
edges = cv2.Canny(gray_image, 50, 150)

# Create a three-channel image with the edges
edges_colored = cv2.cvtColor(edges, cv2.COLOR_GRAY2BGR)

# Combine the original image with the edge-detected image
img_detect = cv2.addWeighted(img_url, 1, edges_colored, 0.7, 0) '''

'''#Another sharpening
kernel = np.array([[-1, -1, -1],
                   [-1, 9, -1],
                   [-1, -1, -1]])
img_detect = cv2.filter2D(img_detect, -1, kernel) '''

'''# Apply Sobel filter for additional sharpening
sobel_x = cv2.Sobel(img_detect, cv2.CV_64F, 1, 0, ksize=3)
sobel_y = cv2.Sobel(img_detect, cv2.CV_64F, 0, 1, ksize=3)
gradient_magnitude = np.sqrt(sobel_x**2 + sobel_y**2)
img_detect = cv2.convertScaleAbs(gradient_magnitude)'''

img_detect = cv2.resize(img_detect, (800, 600))

detection = best_model.predict(img_detect)
detection.show()

"""# **Getting the boundary box for cropping**"""

#original= cv2.imread('211104-JPJ-fancy-plate.jpg', cv2.IMREAD_UNCHANGED)
#inverse_image = 255 - original

#detected_object = best_model.predict(inverse_image)
#detected_object.show()

#detected_object.save(output_folder="output_folder")

prediction_objects = list(detection._images_prediction_lst)[0]
bboxes = prediction_objects.prediction.bboxes_xyxy

int_labels = prediction_objects.prediction.labels.astype(int)
class_names = prediction_objects.class_names
pred_classes = [class_names[i] for i in int_labels]

print("bbox: ", bboxes)

"""# **Cropping**"""

#box = (324.00272 , 238.53203, 731.3148, 351.70703)
bbox = bboxes[0]
#crop_box = (bbox[0], bbox[1], bbox[2], bbox[3])
#img2 = imgdetect.crop(crop_box)

# Convert bounding box coordinates to integers
crop_box = (int(bbox[0]), int(bbox[1]), int(bbox[2]), int(bbox[3]))

# Crop the image
img2 = img_detect[crop_box[1]:crop_box[3], crop_box[0]:crop_box[2]]
#img2 = np.asarray(img2)
img2 = cv2.bitwise_not(img2)

#img2.save('/content/drive/MyDrive/FYP/content/cropped.png')
cv2_imshow(img2)

"""# **Pre-processing**"""

#!sudo apt install tesseract-ocr
#!pip install pytesseract

"""# **Detection**"""

reader = easyocr.Reader(['en'])

# Replace 'your_image_path.jpg' with the path to your image
#image_path = '/content/drive/MyDrive/FYP/content/cropped.jpg'
#img2 = cv2.imread(image_path)

#img2 = cartoon_upsampling_4x(img_path,'/content/super.jpg')

#kernel = np.array([[0, -1, 0], [-1, 5, -1], [0, -1, 0]])
#img2 = cv2.filter2D(img2, -1, kernel)


#img2 = cv2.addWeighted(img2, 1.1, img2, 0, 0)
# Adjust contrast and brightness
alpha = 1.0 # Contrast control (1.0 means no change)
beta = 50    # Brightness control
img2 = cv2.convertScaleAbs(img2, alpha=alpha, beta=beta)

'''#Another sharpening
kernel = np.array([[-1, -1, -1],
                   [-1, 9, -1],
                   [-1, -1, -1]])
img2 = cv2.filter2D(img2, -1, kernel) '''


#Upscaling to improve recognisation
#img2 = cv2.resize(img2, None, fx=2, fy=2, interpolation=cv2.INTER_CUBIC)

'''# Apply Laplacian filter for sharpening
img2 = cv2.Laplacian(img2, cv2.CV_64F)
img2 = cv2.convertScaleAbs(img2)

# Apply bilateral filter for smoothing while preserving edges
img2 = cv2.bilateralFilter(img2, 9, 75, 75) '''

'''#Another sharpening
kernel = np.array([[-1, -1, -1],
                   [-1, 9, -1],
                   [-1, -1, -1]])
img2 = cv2.filter2D(img2, -1, kernel) '''



#image = cv2.resize(image, (800, 600))
cv2_imshow(img2)

"""Trying on pytesseract"""

'''import pytesseract
import shutil
import os
import random

# Replace 'your_image_path.jpg' with the path to your image
image_path = 'your_image_path.jpg'
image = cv2.imread(image_path)

# Convert the image to grayscale
gray_image = cv2.cvtColor(img2, cv2.COLOR_BGR2GRAY)

# Use pytesseract to perform OCR
text = pytesseract.image_to_string(gray_image, lang='eng')

print("Detected Text:")
print(text)'''

# Perform OCR on the image
results = reader.readtext(img2)

# Display the image with bounding boxes around detected characters
for (bbox, text, prob) in results:
    (top_left, top_right, bottom_right, bottom_left) = bbox
    top_left = tuple(map(int, top_left))
    bottom_right = tuple(map(int, bottom_right))
    cv2.rectangle(img2, top_left, bottom_right, (0, 255, 0), 2)
    cv2.putText(img2, f'{text} ({prob:.2f})', top_left, cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 0, 255), 2)

# Display the annotated image
plt.subplot(1, 2, 1)
plt.imshow(cv2.cvtColor(img2, cv2.COLOR_BGR2RGB))
plt.axis('off')

plt.subplot(1, 2, 2)
detected_text = [f'{text} ({prob:.2f})' for (bbox, text, prob) in results]
plt.text(0, 0.5, '\n'.join(detected_text), fontsize=12, verticalalignment='center')
plt.axis('off')


plt.show()



t2 = time.perf_counter()
t3 = t2-t1
#print(t3)
plt.annotate(t3, (0, 0))
plt.axis('off')
plt.show()

#!pip install pytesseract

'''import pytesseract
pytesseract.pytesseract.tesseract_cmd = r/usr/local/lib/python3.10/dist-packages
extracted_text = pytesseract.image_to_string(img2)
print(extracted_text)'''

"""# *Zip* the trained file **bold text**"""

#!zip -r /content/file.zip /content/checkpoints

# save pipe.pkl to output data folder
#!cp -r "/content/License-Plate-1" "/content/drive/MyDrive/FYP"
